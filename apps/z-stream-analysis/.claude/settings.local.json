{
  "permissions": {
    "allow": [
      "Bash(python3:*)",
      "Bash(curl:*)",
      "Bash(git clone:*)",
      "Bash(git checkout:*)",
      "Bash(git -C ./clc-repo-analysis/clc-ui-e2e checkout release-2.12)",
      "WebFetch(domain:jenkins-csb-rhacm-tests.dno.corp.redhat.com)",
      "Bash(mkdir:*)",
      "Bash(grep:*)",
      "WebFetch(domain:github.com)",
      "Bash(rm:*)",
      "Bash(mv:*)",
      "Bash(git -C temp-repo-analysis/ branch --show-current)",
      "Bash(python -m src.scripts.gather:*)",
      "Bash(python:*)",
      "Bash(gh pr view:*)",
      "Bash(gh pr diff:*)",
      "Bash(oc whoami:*)",
      "Bash(oc get:*)",
      "WebFetch(domain:console-openshift-console.apps.slot-03.dev09.red-chesterfield.com)",
      "Bash(oc login:*)",
      "Bash(CONSOLE_URL=https://console-openshift-console.apps.ci-vb-tr35-y.dev09.red-chesterfield.com curl -sk $CONSOLE_URL/)",
      "Bash(git fetch:*)",
      "Bash(CONSOLE_URL=\"https://console-openshift-console.apps.ci-vb-tr35-y.dev09.red-chesterfield.com\")",
      "Bash(echo:*)",
      "Bash(find:*)",
      "Bash(ls:*)",
      "Bash(gh pr list:*)",
      "Bash(cat:*)",
      "Bash(pytest:*)",
      "Bash(pip3 list:*)",
      "WebSearch",
      "Bash(__NEW_LINE_25eb048ef902cb2f__ echo \"\")",
      "Bash(if grep -q \"v2.0\" CLAUDE.md)",
      "Bash(then)",
      "Bash(else)",
      "Bash(fi)",
      "Bash(if grep -q \"automatically cleans up\" .claude/agents/z-stream-analysis.md)",
      "Bash(if grep:*)",
      "Bash(claude mcp:*)",
      "WebFetch(domain:raw.githubusercontent.com)",
      "Bash(iconv:*)",
      "mcp__acm-ui__list_repos",
      "mcp__acm-ui__get_current_version",
      "mcp__acm-ui__get_fleet_virt_selectors",
      "mcp__acm-ui__search_component",
      "mcp__acm-ui__find_test_ids",
      "mcp__acm-ui__search_code",
      "mcp__acm-ui__get_route_component",
      "mcp__acm-ui__get_component_source",
      "mcp__acm-ui__detect_cnv_version",
      "Bash(xargs:*)",
      "mcp__acm-ui__list_versions",
      "mcp__jira__list_teams",
      "mcp__jira__list_component_aliases",
      "Bash(git stash:*)",
      "Bash(cut:*)",
      "mcp__acm-ui__set_acm_version",
      "mcp__jira__search_issues",
      "mcp__acm-ui__get_acm_selectors",
      "mcp__acm-ui__search_translations",
      "mcp__acm-ui__get_routes",
      "mcp__acm-ui__get_patternfly_selectors",
      "mcp__jira__get_issue",
      "Bash(wc:*)",
      "Bash(done)",
      "Read",
      "Edit",
      "Write",
      "Bash(pip3 install:*)",
      "Bash(pip3 show:*)",
      "Bash(chmod:*)",
      "Bash(podman ps:*)",
      "Bash(/tmp/z-stream-test-scenarios/test_report_scenarios.py << 'PYEOF'\n#!/usr/bin/env python3\n\"\"\"\nComprehensive test of report.py changes.\nTests every edge case to ensure no crashes or unexpected behavior.\n\"\"\"\nimport json\nimport os\nimport shutil\nimport sys\nimport tempfile\nfrom pathlib import Path\n\n# Add project root to path\nsys.path.insert\\(0, os.path.dirname\\(os.path.dirname\\(os.path.abspath\\(__file__\\)\\)\\)\\)\n\nPASS_COUNT = 0\nFAIL_COUNT = 0\n\ndef make_core_data\\(\\):\n    \"\"\"Minimal valid core-data.json.\"\"\"\n    return {\n        \"metadata\": {\n            \"jenkins_url\": \"https://jenkins.example.com/job/test/1/\",\n            \"gathered_at\": \"2026-02-11T22:00:00Z\"\n        },\n        \"jenkins\": {\n            \"job_name\": \"test-job\",\n            \"build_number\": 1,\n            \"build_result\": \"UNSTABLE\",\n            \"branch\": \"main\"\n        },\n        \"environment\": {\n            \"cluster_connectivity\": True,\n            \"api_accessibility\": True,\n            \"environment_score\": 1.0,\n            \"cluster_info\": {\"name\": \"test-cluster\", \"platform\": \"OpenShift\"}\n        },\n        \"test_report\": {\n            \"summary\": {\n                \"total_tests\": 10,\n                \"passed_count\": 8,\n                \"failed_count\": 2,\n                \"pass_rate\": 80.0\n            },\n            \"failed_tests\": [\n                {\n                    \"test_name\": \"test_example_1\",\n                    \"class_name\": \"TestSuite\",\n                    \"error_message\": \"Element not found\",\n                    \"failure_type\": \"element_not_found\",\n                    \"preliminary_classification\": \"UNKNOWN\",\n                    \"preliminary_confidence\": 0,\n                    \"preliminary_reasoning\": \"\",\n                    \"preliminary_fix\": \"\"\n                },\n                {\n                    \"test_name\": \"test_example_2\",\n                    \"class_name\": \"TestSuite\",\n                    \"error_message\": \"Timeout\",\n                    \"failure_type\": \"timeout\",\n                    \"preliminary_classification\": \"UNKNOWN\",\n                    \"preliminary_confidence\": 0,\n                    \"preliminary_reasoning\": \"\",\n                    \"preliminary_fix\": \"\"\n                }\n            ]\n        },\n        \"console_log\": {\"total_lines\": 100, \"error_count\": 0, \"key_errors\": []}\n    }\n\n\ndef make_valid_analysis\\(\\):\n    \"\"\"Valid analysis-results.json matching the schema.\"\"\"\n    return {\n        \"investigation_phases_completed\": [\"A\", \"B\", \"C\", \"D\"],\n        \"per_test_analysis\": [\n            {\n                \"test_name\": \"test_example_1\",\n                \"classification\": \"AUTOMATION_BUG\",\n                \"confidence\": 0.90,\n                \"evidence_sources\": [\n                    {\"source\": \"console_search\", \"finding\": \"selector not found\", \"tier\": 1},\n                    {\"source\": \"test_code\", \"finding\": \"stale selector\", \"tier\": 1}\n                ],\n                \"reasoning\": {\n                    \"summary\": \"Selector not found in product\",\n                    \"evidence\": [\"console_search.found=false\"],\n                    \"conclusion\": \"Automation uses outdated selector\"\n                },\n                \"recommended_fix\": {\n                    \"action\": \"Update selector\",\n                    \"steps\": [\"Edit test file\"],\n                    \"owner\": \"Automation Team\"\n                }\n            },\n            {\n                \"test_name\": \"test_example_2\",\n                \"classification\": \"INFRASTRUCTURE\",\n                \"confidence\": 0.85,\n                \"evidence_sources\": [\n                    {\"source\": \"environment\", \"finding\": \"timeout\", \"tier\": 1},\n                    {\"source\": \"console_log\", \"finding\": \"network error\", \"tier\": 2}\n                ],\n                \"reasoning\": \"Network timeout during test execution\"\n            }\n        ],\n        \"summary\": {\n            \"total_failures\": 2,\n            \"by_classification\": {\n                \"AUTOMATION_BUG\": 1,\n                \"INFRASTRUCTURE\": 1\n            },\n            \"overall_classification\": \"MIXED\",\n            \"overall_confidence\": 0.87\n        }\n    }\n\n\ndef setup_run_dir\\(core_data=None, analysis_results=None, manifest=True\\):\n    \"\"\"Create a temporary run directory with given files.\"\"\"\n    run_dir = Path\\(tempfile.mkdtemp\\(\\)\\)\n    \n    if core_data is not None:\n        \\(run_dir / \"core-data.json\"\\).write_text\\(json.dumps\\(core_data, indent=2\\)\\)\n    \n    if manifest:\n        \\(run_dir / \"manifest.json\"\\).write_text\\(json.dumps\\({\"version\": \"2.5\"}\\)\\)\n    \n    if analysis_results is not None:\n        \\(run_dir / \"analysis-results.json\"\\).write_text\\(json.dumps\\(analysis_results, indent=2\\)\\)\n    \n    return run_dir\n\n\ndef cleanup\\(run_dir\\):\n    \"\"\"Remove temp directory.\"\"\"\n    shutil.rmtree\\(run_dir, ignore_errors=True\\)\n\n\ndef test\\(name, fn\\):\n    \"\"\"Run a test function, report pass/fail.\"\"\"\n    global PASS_COUNT, FAIL_COUNT\n    try:\n        fn\\(\\)\n        print\\(f\"  PASS: {name}\"\\)\n        PASS_COUNT += 1\n    except Exception as e:\n        print\\(f\"  FAIL: {name}\"\\)\n        print\\(f\"        {type\\(e\\).__name__}: {str\\(e\\)[:200]}\"\\)\n        FAIL_COUNT += 1\n\n\n# ============================================================\n# Import report module\n# ============================================================\nos.chdir\\(\"/Users/ashafi/Documents/work/ai/ai_systems_v2/apps/z-stream-analysis\"\\)\nsys.path.insert\\(0, \".\"\\)\nfrom src.scripts.report import ReportFormatter\n\n\n# ============================================================\n# SCENARIO 1: No analysis-results.json \\(Stage 2 not run yet\\)\n# Should generate report from raw data without crashing.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 1: No analysis-results.json ===\"\\)\n\ndef test_no_analysis_results\\(\\):\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=None\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        assert \\(run_dir / \"Detailed-Analysis.md\"\\).exists\\(\\), \"Markdown report not created\"\n        assert \\(run_dir / \"SUMMARY.txt\"\\).exists\\(\\), \"Summary not created\"\n        assert \\(run_dir / \"per-test-breakdown.json\"\\).exists\\(\\), \"JSON breakdown not created\"\n        # Should contain the raw data fallback section\n        md = \\(run_dir / \"Detailed-Analysis.md\"\\).read_text\\(\\)\n        assert \"Individual Test Failures\" in md, \"Should show raw test failures\"\n        assert \"AI analysis not yet performed\" in md, \"Should show fallback note\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"No analysis-results.json → raw data fallback\", test_no_analysis_results\\)\n\n\n# ============================================================\n# SCENARIO 2: Valid analysis-results.json\n# Should generate full report with all sections.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 2: Valid analysis-results.json ===\"\\)\n\ndef test_valid_analysis\\(\\):\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=make_valid_analysis\\(\\)\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        md = \\(run_dir / \"Detailed-Analysis.md\"\\).read_text\\(\\)\n        assert \"AUTOMATION BUG\" in md, \"Should show AUTOMATION_BUG classification\"\n        assert \"INFRASTRUCTURE\" in md, \"Should show INFRASTRUCTURE classification\"\n        assert \"test_example_1\" in md, \"Should show test name\"\n        assert \"Update selector\" in md, \"Should show recommended fix\"\n        summary = \\(run_dir / \"SUMMARY.txt\"\\).read_text\\(\\)\n        assert \"MIXED\" in summary, \"Summary should show overall classification\"\n        assert \"87%\" in summary, \"Summary should show confidence\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Valid analysis-results.json → full report\", test_valid_analysis\\)\n\n\n# ============================================================\n# SCENARIO 3: Wrong field names \\(the bug we're fixing\\)\n# Should fail with clear error, NOT silently produce empty report.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 3: Wrong field names \\(old bug\\) ===\"\\)\n\ndef test_wrong_field_names\\(\\):\n    bad_analysis = {\n        \"failed_tests\": [  # WRONG: should be per_test_analysis\n            {\"test_name\": \"test1\", \"classification\": \"AUTOMATION_BUG\", \"confidence\": 0.9,\n             \"evidence_sources\": [{\"source\": \"a\", \"finding\": \"b\"}, {\"source\": \"c\", \"finding\": \"d\"}]}\n        ],\n        \"summary\": {\n            \"classification_breakdown\": {\"AUTOMATION_BUG\": 1}  # WRONG: should be by_classification\n        }\n    }\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=bad_analysis\\)\n    try:\n        raised = False\n        try:\n            formatter = ReportFormatter\\(run_dir\\)\n        except ValueError as e:\n            raised = True\n            msg = str\\(e\\)\n            assert \"per_test_analysis\" in msg, \"Error should mention per_test_analysis\"\n            assert \"schema\" in msg.lower\\(\\), \"Error should reference schema\"\n        assert raised, \"Should have raised ValueError for wrong field names\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Wrong field names → ValueError with guidance\", test_wrong_field_names\\)\n\n\n# ============================================================\n# SCENARIO 4: analysis-results.json exists but is empty object\n# Should fail with clear error.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 4: Empty analysis-results.json \\({}\\) ===\"\\)\n\ndef test_empty_analysis\\(\\):\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results={}\\)\n    try:\n        raised = False\n        try:\n            formatter = ReportFormatter\\(run_dir\\)\n        except ValueError as e:\n            raised = True\n            assert \"per_test_analysis\" in str\\(e\\), \"Should mention missing per_test_analysis\"\n        assert raised, \"Should have raised ValueError for empty analysis\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Empty {} analysis-results.json → ValueError\", test_empty_analysis\\)\n\n\n# ============================================================\n# SCENARIO 5: per_test_analysis exists but is empty array\n# Should fail with clear error \\(no tests analyzed\\).\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 5: Empty per_test_analysis array ===\"\\)\n\ndef test_empty_per_test_array\\(\\):\n    analysis = {\n        \"investigation_phases_completed\": [\"A\"],\n        \"per_test_analysis\": [],  # Empty array\n        \"summary\": {\n            \"by_classification\": {\"AUTOMATION_BUG\": 0},\n            \"overall_classification\": \"UNKNOWN\",\n            \"overall_confidence\": 0\n        }\n    }\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        raised = False\n        try:\n            formatter.format_all\\(\\)\n        except ValueError as e:\n            raised = True\n            assert \"per_test_analysis\" in str\\(e\\), \"Should mention per_test_analysis\"\n            assert \"missing or empty\" in str\\(e\\), \"Should say missing or empty\"\n        assert raised, \"Should have raised ValueError for empty per_test_analysis\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Empty per_test_analysis [] → ValueError on format\", test_empty_per_test_array\\)\n\n\n# ============================================================\n# SCENARIO 6: validate_schema=False bypasses validation\n# Should still work for programmatic use cases.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 6: validate_schema=False bypass ===\"\\)\n\ndef test_validation_bypass\\(\\):\n    bad_analysis = {\n        \"failed_tests\": [{\"test_name\": \"t1\"}],\n        \"summary\": {\"classification_breakdown\": {}}\n    }\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=bad_analysis\\)\n    try:\n        # Should NOT raise when validation is disabled\n        formatter = ReportFormatter\\(run_dir, validate_schema=False\\)\n        # But format_all will still fail because per_test_analysis is missing\n        raised = False\n        try:\n            formatter.format_all\\(\\)\n        except ValueError:\n            raised = True\n        assert raised, \"format_all should still fail even with validation disabled\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"validate_schema=False → init succeeds, format still catches\", test_validation_bypass\\)\n\n\n# ============================================================\n# SCENARIO 7: Valid analysis with minimal fields \\(no optional fields\\)\n# Should work fine — only required fields present.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 7: Minimal valid analysis \\(no optional fields\\) ===\"\\)\n\ndef test_minimal_valid\\(\\):\n    analysis = {\n        \"investigation_phases_completed\": [\"A\", \"B\", \"C\", \"D\"],\n        \"per_test_analysis\": [\n            {\n                \"test_name\": \"test1\",\n                \"classification\": \"PRODUCT_BUG\",\n                \"confidence\": 0.80,\n                \"evidence_sources\": [\n                    {\"source\": \"console_log\", \"finding\": \"500 error\"},\n                    {\"source\": \"environment\", \"finding\": \"env healthy\"}\n                ]\n                # No reasoning, recommended_fix, owner, priority — all optional\n            }\n        ],\n        \"summary\": {\n            \"by_classification\": {\"PRODUCT_BUG\": 1},\n            \"overall_classification\": \"PRODUCT_BUG\",\n            \"overall_confidence\": 0.80\n        }\n    }\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        md = \\(run_dir / \"Detailed-Analysis.md\"\\).read_text\\(\\)\n        assert \"PRODUCT BUG\" in md, \"Should show classification\"\n        assert \"test1\" in md, \"Should show test name\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Minimal valid analysis → report generated\", test_minimal_valid\\)\n\n\n# ============================================================\n# SCENARIO 8: No core-data.json at all\n# Should raise FileNotFoundError \\(pre-existing behavior\\).\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 8: No core-data.json ===\"\\)\n\ndef test_no_core_data\\(\\):\n    run_dir = Path\\(tempfile.mkdtemp\\(\\)\\)\n    try:\n        raised = False\n        try:\n            formatter = ReportFormatter\\(run_dir\\)\n        except FileNotFoundError:\n            raised = True\n        assert raised, \"Should raise FileNotFoundError when no data files exist\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"No core-data.json → FileNotFoundError\", test_no_core_data\\)\n\n\n# ============================================================\n# SCENARIO 9: analysis-results.json with invalid JSON\n# Should handle gracefully \\(json.load fails\\).\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 9: Malformed JSON in analysis-results.json ===\"\\)\n\ndef test_malformed_json\\(\\):\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\)\\)\n    \\(run_dir / \"analysis-results.json\"\\).write_text\\(\"{invalid json content\"\\)\n    try:\n        raised = False\n        try:\n            formatter = ReportFormatter\\(run_dir\\)\n        except json.JSONDecodeError:\n            raised = True\n        assert raised, \"Should raise JSONDecodeError for malformed JSON\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Malformed JSON → JSONDecodeError\", test_malformed_json\\)\n\n\n# ============================================================\n# SCENARIO 10: analysis-results.json is a JSON array not object\n# Should handle gracefully.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 10: analysis-results.json is array, not object ===\"\\)\n\ndef test_json_array\\(\\):\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\)\\)\n    \\(run_dir / \"analysis-results.json\"\\).write_text\\('[{\"test\": \"data\"}]'\\)\n    try:\n        # _load_json returns the array, but validation will fail because it's not a dict\n        raised = False\n        try:\n            formatter = ReportFormatter\\(run_dir\\)\n        except \\(ValueError, TypeError, AttributeError\\) as e:\n            raised = True\n        assert raised, \"Should raise error for non-object analysis results\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"JSON array instead of object → error\", test_json_array\\)\n\n\n# ============================================================\n# SCENARIO 11: Valid analysis with string reasoning \\(not object\\)\n# Should work — reasoning can be string or object per schema.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 11: String reasoning \\(not object\\) ===\"\\)\n\ndef test_string_reasoning\\(\\):\n    analysis = make_valid_analysis\\(\\)\n    analysis[\"per_test_analysis\"][0][\"reasoning\"] = \"Simple string explanation\"\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        md = \\(run_dir / \"Detailed-Analysis.md\"\\).read_text\\(\\)\n        assert \"Simple string explanation\" in md, \"Should render string reasoning\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"String reasoning → rendered correctly\", test_string_reasoning\\)\n\n\n# ============================================================\n# SCENARIO 12: Valid analysis with string recommended_fix \\(not object\\)\n# Should work — recommended_fix can be string or object per schema.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 12: String recommended_fix \\(not object\\) ===\"\\)\n\ndef test_string_fix\\(\\):\n    analysis = make_valid_analysis\\(\\)\n    analysis[\"per_test_analysis\"][0][\"recommended_fix\"] = \"Update the selector to #new-btn\"\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        md = \\(run_dir / \"Detailed-Analysis.md\"\\).read_text\\(\\)\n        assert \"Update the selector to #new-btn\" in md, \"Should render string fix\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"String recommended_fix → rendered correctly\", test_string_fix\\)\n\n\n# ============================================================\n# SCENARIO 13: Legacy raw-data.json \\(no core-data.json\\)\n# Should still work with legacy format.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 13: Legacy raw-data.json format ===\"\\)\n\ndef test_legacy_format\\(\\):\n    run_dir = Path\\(tempfile.mkdtemp\\(\\)\\)\n    raw_data = make_core_data\\(\\)\n    \\(run_dir / \"raw-data.json\"\\).write_text\\(json.dumps\\(raw_data, indent=2\\)\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        assert \\(run_dir / \"Detailed-Analysis.md\"\\).exists\\(\\)\n        md = \\(run_dir / \"Detailed-Analysis.md\"\\).read_text\\(\\)\n        assert \"Individual Test Failures\" in md\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Legacy raw-data.json → fallback works\", test_legacy_format\\)\n\n\n# ============================================================\n# SCENARIO 14: Valid analysis with many tests \\(performance check\\)\n# Should not crash with large number of tests.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 14: Large analysis \\(50 tests\\) ===\"\\)\n\ndef test_large_analysis\\(\\):\n    analysis = make_valid_analysis\\(\\)\n    base_test = analysis[\"per_test_analysis\"][0]\n    analysis[\"per_test_analysis\"] = []\n    for i in range\\(50\\):\n        t = dict\\(base_test\\)\n        t[\"test_name\"] = f\"test_large_{i}\"\n        analysis[\"per_test_analysis\"].append\\(t\\)\n    analysis[\"summary\"][\"total_failures\"] = 50\n    analysis[\"summary\"][\"by_classification\"][\"AUTOMATION_BUG\"] = 50\n    \n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        md = \\(run_dir / \"Detailed-Analysis.md\"\\).read_text\\(\\)\n        assert \"test_large_49\" in md, \"Should include all 50 tests\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"50 tests → all rendered without crash\", test_large_analysis\\)\n\n\n# ============================================================\n# SCENARIO 15: Valid analysis with special characters in test names\n# Should not crash with pipes, backticks, newlines in data.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 15: Special characters in test data ===\"\\)\n\ndef test_special_chars\\(\\):\n    analysis = make_valid_analysis\\(\\)\n    analysis[\"per_test_analysis\"][0][\"test_name\"] = 'test with | pipes and `backticks` and \"quotes\"'\n    analysis[\"per_test_analysis\"][0][\"reasoning\"] = {\n        \"summary\": \"Error: can't find <element> in #container | undefined\",\n        \"evidence\": [\"line with | pipe\", \"line with `backtick`\"],\n        \"conclusion\": \"Test uses | character in selector\"\n    }\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        assert \\(run_dir / \"Detailed-Analysis.md\"\\).exists\\(\\)\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Special chars in data → no crash\", test_special_chars\\)\n\n\n# ============================================================\n# SCENARIO 16: Has per_test_analysis but missing summary.by_classification\n# Schema validation should catch this at init.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 16: Has per_test but missing by_classification ===\"\\)\n\ndef test_missing_by_classification\\(\\):\n    analysis = make_valid_analysis\\(\\)\n    del analysis[\"summary\"][\"by_classification\"]\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        raised = False\n        try:\n            formatter = ReportFormatter\\(run_dir\\)\n        except ValueError as e:\n            raised = True\n            assert \"by_classification\" in str\\(e\\)\n        assert raised, \"Should fail on missing by_classification\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Missing by_classification → ValueError\", test_missing_by_classification\\)\n\n\n# ============================================================\n# SCENARIO 17: Has summary but missing per_test_analysis\n# Schema validation should catch this at init.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 17: Has summary but no per_test_analysis ===\"\\)\n\ndef test_missing_per_test\\(\\):\n    analysis = {\n        \"summary\": {\n            \"by_classification\": {\"AUTOMATION_BUG\": 1},\n            \"overall_classification\": \"AUTOMATION_BUG\",\n            \"overall_confidence\": 0.9\n        }\n    }\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=analysis\\)\n    try:\n        raised = False\n        try:\n            formatter = ReportFormatter\\(run_dir\\)\n        except ValueError as e:\n            raised = True\n            assert \"per_test_analysis\" in str\\(e\\)\n        assert raised, \"Should fail on missing per_test_analysis\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"Missing per_test_analysis → ValueError at init\", test_missing_per_test\\)\n\n\n# ============================================================\n# SCENARIO 18: CLI entry point with bad analysis\n# Should exit with code 1, not crash with traceback.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 18: CLI exit behavior ===\"\\)\n\ndef test_cli_exit\\(\\):\n    import subprocess\n    bad_analysis = {\"failed_tests\": [], \"summary\": {}}\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=bad_analysis\\)\n    try:\n        result = subprocess.run\\(\n            [sys.executable, \"-m\", \"src.scripts.report\", str\\(run_dir\\)],\n            capture_output=True, text=True,\n            cwd=\"/Users/ashafi/Documents/work/ai/ai_systems_v2/apps/z-stream-analysis\"\n        \\)\n        assert result.returncode == 1, f\"Should exit with code 1, got {result.returncode}\"\n        assert \"Error:\" in result.stderr or \"Error:\" in result.stdout, \"Should show Error message\"\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"CLI with bad analysis → exit code 1\", test_cli_exit\\)\n\n\n# ============================================================\n# SCENARIO 19: CLI with valid analysis\n# Should exit with code 0.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 19: CLI with valid analysis ===\"\\)\n\ndef test_cli_valid\\(\\):\n    import subprocess\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\), analysis_results=make_valid_analysis\\(\\)\\)\n    try:\n        result = subprocess.run\\(\n            [sys.executable, \"-m\", \"src.scripts.report\", str\\(run_dir\\)],\n            capture_output=True, text=True,\n            cwd=\"/Users/ashafi/Documents/work/ai/ai_systems_v2/apps/z-stream-analysis\"\n        \\)\n        assert result.returncode == 0, f\"Should exit with code 0, got {result.returncode}\\\\nstderr: {result.stderr}\"\n        assert \\(run_dir / \"Detailed-Analysis.md\"\\).exists\\(\\)\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"CLI with valid analysis → exit code 0\", test_cli_valid\\)\n\n\n# ============================================================\n# SCENARIO 20: analysis-results.json is null \\(json null\\)\n# Should handle without crash.\n# ============================================================\nprint\\(\"\\\\n=== SCENARIO 20: analysis-results.json contains 'null' ===\"\\)\n\ndef test_json_null\\(\\):\n    run_dir = setup_run_dir\\(core_data=make_core_data\\(\\)\\)\n    \\(run_dir / \"analysis-results.json\"\\).write_text\\(\"null\"\\)\n    try:\n        # _load_json returns None for null, so analysis_results is None\n        # Should behave like no analysis file\n        formatter = ReportFormatter\\(run_dir\\)\n        reports = formatter.format_all\\(\\)\n        assert \\(run_dir / \"Detailed-Analysis.md\"\\).exists\\(\\)\n    finally:\n        cleanup\\(run_dir\\)\n\ntest\\(\"JSON null → treated as no analysis\", test_json_null\\)\n\n\n# ============================================================\n# SUMMARY\n# ============================================================\nprint\\(\"\\\\n\" + \"=\" * 60\\)\nprint\\(f\"RESULTS: {PASS_COUNT} passed, {FAIL_COUNT} failed out of {PASS_COUNT + FAIL_COUNT} tests\"\\)\nprint\\(\"=\" * 60\\)\n\nif FAIL_COUNT > 0:\n    sys.exit\\(1\\)\nPYEOF)",
      "Bash(__NEW_LINE_7107fba8bf600523__ python3 /tmp/z-stream-test-scenarios/test_report_scenarios.py)"
    ],
    "deny": []
  }
}
